{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Examples of text vectorization techniques\n",
    "\n",
    "This notebooks presents description and examples of NLP techniques that can be used for text vectorization.\n",
    "\n",
    "Based on [Python Machine Learning (2nd edition)](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939) by Sebastian Raschka and Vahid Mirjalili, [Applied Text Analysis with Python](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html) by Tony Ojeda, Rebecca Bilbro, Benjamin Bengfort, and [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of words (BoW) \n",
    "\n",
    "### Unigrams\n",
    "In case of unigrams, each term is an individual word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 6,\n 'sun': 4,\n 'is': 1,\n 'shining': 3,\n 'weather': 8,\n 'sweet': 5,\n 'and': 0,\n 'one': 2,\n 'two': 7}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, '\n",
    "                 'and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "count.vocabulary_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "bag.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### n-grams\n",
    "Contiguous sequences of items in NLP—words, letters, or symbols—are also called n-grams.\n",
    "The choice of the number n in the n-gram model depends on the particular application.\n",
    "\n",
    "Character n-grams could also be used as a representation of words, to avoid the use of tokenizers, which could be beneficial for such applications as email anti-spam filtering (since spammers attempt to confuse tokenizers) even if it increases the dimensionality of a problem, as shown in the study by [Kanaris et al.](https://www.researchgate.net/publication/220160318_Words_versus_Character_n-Grams_for_Anti-Spam_Filtering)\n",
    "### Word bigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the sun': 9,\n 'sun is': 7,\n 'is shining': 1,\n 'the weather': 10,\n 'weather is': 11,\n 'is sweet': 2,\n 'shining the': 6,\n 'sweet and': 8,\n 'and one': 0,\n 'one and': 4,\n 'one is': 5,\n 'is two': 3}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(2,2))\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, '\n",
    "                 'and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "count.vocabulary_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n       [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "bag.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "When analyzing text data, it is common to encounter words that occur across multiple documents from both classes.\n",
    "These frequently occurring words typically don't contain useful or discriminatory information.\n",
    "\n",
    "Term frequency-inverse document frequency (tf-idf) method can be used to downweight frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "$tf\\_idf(t, d) = tf(t, d) \\times idf(t, d)$\n",
    "\n",
    "Here the $tf(t, d)$ is the term frequency introduced for Bag of Words, and $idf(t, d)$ is the inverse document frequency and can be calculated as follows:\n",
    "\n",
    "$idf(t, d) = \\log \\large{ \\frac{n_d} {1 + df(d, t)} }$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}