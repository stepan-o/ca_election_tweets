@book{Bengfort2018,
    abstract = {From news and speeches to informal chatter on social media, natural language is one of the richest and most underutilized sources of data. Not only does it come in a constant stream, always changing and adapting in context; it also contains information that is not conveyed by traditional data sources. The key to unlocking natural language is through the creative application of text analytics. This practical book presents a data scientist's approach to building language-aware products with applied machine learning. You'll learn robust, repeatable, and scalable techniques for text analysis with Python, including contextual and linguistic feature engineering, vectorization, classification, topic modeling, entity resolution, graph analysis, and visual steering. By the end of the book, you'll be equipped with practical methods to solve any number of complex real-world problems. Preprocess and vectorize text into high-dimensional feature representations Perform document classification and topic modeling Steer the model selection process with visual diagnostics Extract key phrases, named entities, and graph structures to reason about data in text Build a dialog framework to enable chatbots and language-driven interaction Use Spark to scale processing power and neural networks to scale model complexityFrom news and speeches to informal chatter on social media, natural language is one of the richest and most underutilized sources of data. Not only does it come in a constant stream, always changing and adapting in context; it also contains information that is not conveyed by traditional data sources. The key to unlocking natural language is through the creative application of text analytics. This practical book presents a data scientist's approach to building language-aware products with applied machine learning. You'll learn robust, repeatable, and scalable techniques for text analysis with Python, including contextual and linguistic feature engineering, vectorization, classification, topic modeling, entity resolution, graph analysis, and visual steering. By the end of the book, you'll be equipped with practical methods to solve any number of complex real-world problems. Preprocess and vectorize text into high-dimensional feature representations Perform document classification and topic modeling Steer the model selection process with visual diagnostics Extract key phrases, named entities, and graph structures to reason about data in text Build a dialog framework to enable chatbots and language-driven interaction Use Spark to scale processing power and neural networks to scale model complexity},
    author = {Bengfort, Benjamin and Ojeda, Tony and Bilbro, Rebecca},
    pages = {1--332},
    publisher = {O'Reilly Media},
    title = {{ Applied Text Analysis with Python }},
    url = {https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html},
    year = {2018}
}
@article{Kanaris2007,
    abstract = {The increasing number of unsolicited e-mail messages (spam) reveals the need for the development of reliable anti-spam filters. The vast majority of content-based techniques rely on word-based representation of messages. Such approaches require reliable tokenizers for detecting the token boundaries. As a consequence, a common practice of spammers is to attempt to confuse tokenizers using unexpected punctuation marks or special characters within the message. In this paper we explore an alternative low-level representation based on character n-grams which avoids the use of tokenizers and other language-dependent tools. Based on experiments on two well-known benchmark corpora and a variety of evaluation measures, we show that character n-grams are more reliable features than word-tokens despite the fact that they increase the dimensionality of the problem. Moreover, we propose a method for extracting variable-length n-grams which produces optimal classifiers among the examined models under cost-sensitive evaluation.},
    author = {Kanaris, Ioannis and Kanaris, Konstantinos and Houvardas, Ioannis and Stamatatos, Efstathios},
    doi = {10.1142/S0218213007003692},
    file = {:home/stepan/repos/ca { \_ } election { \_ } tweets/methodology/references/Kanaris (2007) Words versus Character n-Grams for Anti-Spam Filtering.pdf:pdf},
    issn = {02182130},
    journal = {International Journal on Artificial Intelligence Tools},
    keywords = {Anti-spam filtering,Machine learning,N-grams},
    number = {6},
    pages = {1047--1067},
    title = {{ Words versus character n-grams for anti-spam filtering }},
    volume = {16},
    year = {2007}
}
@book{RaschkaMirjalili2017,
    address = {Birmingham, UK},
    author = {Raschka, Sebastian and Mirjalili, Vahid},
    edition = {2},
    isbn = {978-1787125933},
    keywords = {Clustering,Data Science,Deep Learning,Machine Learning,Neural Networks,Programming,Supervised Learning},
    publisher = {Packt Publishing},
    title = {{ Python Machine Learning, 2nd Ed. }},
    year = {2017}
}
@misc{Scikit-learndevelopers2019,
    abstract = {The sklearn.feature { \_ } extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.},
    author = {Scikit-learn developers},
    booktitle = {Online Documentation for scikit-learn},
    title = {{ Feature extraction }},
    url = {https://scikit-learn.org/stable/modules/feature { \_ } extraction.html},
    year = {2019}
}
