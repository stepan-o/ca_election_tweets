{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Canadian Election tweets\n",
    "# OSEMN Step 3: Explore\n",
    "# Sentiment analysis of Sentiment 140 dataset\n",
    "# Comparison of different text tokenization and vectorization techniques\n",
    "\n",
    "This notebook describes part of Step 3: Explore of OSEMN methodology. It covers exploration of different text vectorization techniques on Sentiment 140 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set()\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git', '.gitignore', 'src', 'notebooks', 'methodology', 'README.md', 'data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../src')\n",
    "from proc_utils import string_concat, tfm_2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testdata.manual.2009.06.14.csv',\n",
       " 'training.1600000.processed.noemoticon.csv',\n",
       " 'sentiment140_train_nodup.csv',\n",
       " 'sentiment140_train_cleaned.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../data/sentiment140/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned Sentiment 140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- DataFrame loaded\n",
      "in 4.49 seconds\n",
      "with 1,309,540 rows\n",
      "and 8 columns\n",
      "-- Column names:\n",
      " Index(['sentiment', 'ids', 'date', 'query', 'user', 'text', 'hashtags',\n",
      "       'handles'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "df = pd.read_csv(data_dir + 'sentiment140_train_nodup.csv')\n",
    "elapsed = time() - t\n",
    "print(\"----- DataFrame loaded\"\n",
    "      \"\\nin {0:.2f} seconds\".format(elapsed) +\n",
    "      \"\\nwith {0:,} rows\\nand {1:,} columns\"\n",
    "      .format(df.shape[0], df.shape[1]) +\n",
    "      \"\\n-- Column names:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split documents into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 4.79 seconds (0.08 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                             [switchfoot, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d;D]\n",
       "1    [is, upset, that, he, can, t, update, his, facebook, by, texting, it, and, might, cry, as, a, result, school, today, also, blah]\n",
       "2                            [kenichan, i, dived, many, times, for, the, ball, managed, to, save, 50, the, rest, go, out, of, bounds]\n",
       "3                                                                           [my, whole, body, feels, itchy, and, like, its, on, fire]\n",
       "4                                                                                                   [kwesidei, not, the, whole, crew]\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['token'] = df['text'].apply(tokenizer)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['token'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535            [just, woke, up, having, no, school, is, the, best, feeling, ever]\n",
       "1309536                    [thewdb, com, very, cool, to, hear, old, walt, interviews]\n",
       "1309537           [are, you, ready, for, your, mojo, makeover, ask, me, for, details]\n",
       "1309538    [happy, 38th, birthday, to, my, boo, of, alll, time, tupac, amaru, shakur]\n",
       "1309539               [happy, charitytuesday, thenspcc, sparkscharity, speakinguph4h]\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'thus', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "def tokenizer_sw(text):\n",
    "    return [w for w in text.split() if w not in stop]\n",
    "tokenizer_sw('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 54.44 seconds (0.91 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0               [switchfoot, awww, bummer, shoulda, got, david, carr, third, day, d;D]\n",
       "1    [upset, update, facebook, texting, might, cry, result, school, today, also, blah]\n",
       "2            [kenichan, dived, many, times, ball, managed, save, 50, rest, go, bounds]\n",
       "3                                              [whole, body, feels, itchy, like, fire]\n",
       "4                                                              [kwesidei, whole, crew]\n",
       "Name: token_sw, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['token_sw'] = df['text'].apply(tokenizer_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['token_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                                [woke, school, best, feeling, ever]\n",
       "1309536                   [thewdb, com, cool, hear, old, walt, interviews]\n",
       "1309537                              [ready, mojo, makeover, ask, details]\n",
       "1309538     [happy, 38th, birthday, boo, alll, time, tupac, amaru, shakur]\n",
       "1309539    [happy, charitytuesday, thenspcc, sparkscharity, speakinguph4h]\n",
       "Name: token_sw, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_sw'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 401.12 seconds (6.69 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                        [switchfoot, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d;d]\n",
       "1    [is, upset, that, he, can, t, updat, hi, facebook, by, text, it, and, might, cri, as, a, result, school, today, also, blah]\n",
       "2                            [kenichan, i, dive, mani, time, for, the, ball, manag, to, save, 50, the, rest, go, out, of, bound]\n",
       "3                                                                        [my, whole, bodi, feel, itchi, and, like, it, on, fire]\n",
       "4                                                                                              [kwesidei, not, the, whole, crew]\n",
       "Name: porter, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['porter'] = df['text'].apply(tokenizer_porter)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['porter'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                 [just, woke, up, have, no, school, is, the, best, feel, ever]\n",
       "1309536                     [thewdb, com, veri, cool, to, hear, old, walt, interview]\n",
       "1309537              [are, you, readi, for, your, mojo, makeov, ask, me, for, detail]\n",
       "1309538    [happi, 38th, birthday, to, my, boo, of, alll, time, tupac, amaru, shakur]\n",
       "1309539                  [happi, charitytuesday, thenspcc, sparkschar, speakinguph4h]\n",
       "Name: porter, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['porter'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Porter stemmer with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(tokens):\n",
    "    return [w for w in tokens if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed, took 51.11 seconds (0.85 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0               [switchfoot, awww, bummer, shoulda, got, david, carr, third, day, d;d]\n",
       "1    [upset, updat, hi, facebook, text, might, cri, result, school, today, also, blah]\n",
       "2                 [kenichan, dive, mani, time, ball, manag, save, 50, rest, go, bound]\n",
       "3                                               [whole, bodi, feel, itchi, like, fire]\n",
       "4                                                              [kwesidei, whole, crew]\n",
       "Name: porter_sw, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['porter_sw'] = df['porter'].apply(remove_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Stop words removed, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['porter_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                                  [woke, school, best, feel, ever]\n",
       "1309536             [thewdb, com, veri, cool, hear, old, walt, interview]\n",
       "1309537                                [readi, mojo, makeov, ask, detail]\n",
       "1309538    [happi, 38th, birthday, boo, alll, time, tupac, amaru, shakur]\n",
       "1309539      [happi, charitytuesday, thenspcc, sparkschar, speakinguph4h]\n",
       "Name: porter_sw, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['porter_sw'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Snowball stemmer (ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball = EnglishStemmer()\n",
    "def tokenizer_snowball(text):\n",
    "    return [snowball.stem(word) for word in text.split()]\n",
    "tokenizer_snowball('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 185.58 seconds (3.09 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         [switchfoot, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d;d]\n",
       "1    [is, upset, that, he, can, t, updat, his, facebook, by, text, it, and, might, cri, as, a, result, school, today, also, blah]\n",
       "2                             [kenichan, i, dive, mani, time, for, the, ball, manag, to, save, 50, the, rest, go, out, of, bound]\n",
       "3                                                                        [my, whole, bodi, feel, itchi, and, like, its, on, fire]\n",
       "4                                                                                               [kwesidei, not, the, whole, crew]\n",
       "Name: snowball, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['snowball'] = df['text'].apply(tokenizer_snowball)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['snowball'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535               [just, woke, up, having, no, school, is, the, best, feel, ever]\n",
       "1309536                     [thewdb, com, very, cool, to, hear, old, walt, interview]\n",
       "1309537              [are, you, readi, for, your, mojo, makeov, ask, me, for, detail]\n",
       "1309538    [happi, 38th, birthday, to, my, boo, of, alll, time, tupac, amaru, shakur]\n",
       "1309539                  [happi, charitytuesday, thenspcc, sparkschar, speakinguph4h]\n",
       "Name: snowball, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['snowball'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Snowball stemmer (ingore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_sw = EnglishStemmer(ignore_stopwords=True)\n",
    "def tokenizer_snowball_sw(text):\n",
    "    return [snowball_sw.stem(word) for word in text.split()]\n",
    "tokenizer_snowball_sw('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 181.98 seconds (3.03 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         [switchfoot, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d;d]\n",
       "1    [is, upset, that, he, can, t, updat, his, facebook, by, text, it, and, might, cri, as, a, result, school, today, also, blah]\n",
       "2                             [kenichan, i, dive, mani, time, for, the, ball, manag, to, save, 50, the, rest, go, out, of, bound]\n",
       "3                                                                        [my, whole, bodi, feel, itchi, and, like, its, on, fire]\n",
       "4                                                                                               [kwesidei, not, the, whole, crew]\n",
       "Name: snowball_sw, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['snowball_sw'] = df['text'].apply(tokenizer_snowball_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['snowball_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535               [just, woke, up, having, no, school, is, the, best, feel, ever]\n",
       "1309536                     [thewdb, com, very, cool, to, hear, old, walt, interview]\n",
       "1309537              [are, you, readi, for, your, mojo, makeov, ask, me, for, detail]\n",
       "1309538    [happi, 38th, birthday, to, my, boo, of, alll, time, tupac, amaru, shakur]\n",
       "1309539                  [happi, charitytuesday, thenspcc, sparkschar, speakinguph4h]\n",
       "Name: snowball_sw, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['snowball_sw'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Snowball stemmer with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed, took 51.34 seconds (0.86 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           [switchfoot, awww, bummer, shoulda, got, david, carr, third, day, d;d]\n",
       "1    [upset, updat, facebook, text, might, cri, result, school, today, also, blah]\n",
       "2             [kenichan, dive, mani, time, ball, manag, save, 50, rest, go, bound]\n",
       "3                                           [whole, bodi, feel, itchi, like, fire]\n",
       "4                                                          [kwesidei, whole, crew]\n",
       "Name: snowball_sw, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['snowball_sw'] = df['snowball'].apply(remove_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Stop words removed, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['snowball_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                                  [woke, school, best, feel, ever]\n",
       "1309536                   [thewdb, com, cool, hear, old, walt, interview]\n",
       "1309537                                [readi, mojo, makeov, ask, detail]\n",
       "1309538    [happi, 38th, birthday, boo, alll, time, tupac, amaru, shakur]\n",
       "1309539      [happi, charitytuesday, thenspcc, sparkschar, speakinguph4h]\n",
       "Name: snowball_sw, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['snowball_sw'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'lik', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "def tokenizer_lancaster(text):\n",
    "    return [lancaster.stem(word) for word in text.split()]\n",
    "tokenizer_lancaster('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 383.00 seconds (6.38 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                            [switchfoot, awww, that, s, a, bum, you, should, got, david, car, of, third, day, to, do, it, d;d]\n",
       "1    [is, upset, that, he, can, t, upd, his, facebook, by, text, it, and, might, cry, as, a, result, school, today, also, blah]\n",
       "2                                   [kenich, i, div, many, tim, for, the, bal, man, to, sav, 50, the, rest, go, out, of, bound]\n",
       "3                                                                          [my, whol, body, feel, itchy, and, lik, it, on, fir]\n",
       "4                                                                                               [kweside, not, the, whol, crew]\n",
       "Name: lancaster, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['lancaster'] = df['text'].apply(tokenizer_lancaster)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['lancaster'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                 [just, wok, up, hav, no, school, is, the, best, feel, ev]\n",
       "1309536                 [thewdb, com, very, cool, to, hear, old, walt, interview]\n",
       "1309537             [ar, you, ready, for, yo, mojo, makeov, ask, me, for, detail]\n",
       "1309538    [happy, 38th, birthday, to, my, boo, of, all, tim, tupac, amaru, shak]\n",
       "1309539                [happy, charitytuesday, thenspcc, sparksch, speakinguph4h]\n",
       "Name: lancaster, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lancaster'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lancaster stemmer with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed, took 48.42 seconds (0.81 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                      [switchfoot, awww, bum, got, david, car, third, day, d;d]\n",
       "1    [upset, upd, facebook, text, might, cry, result, school, today, also, blah]\n",
       "2                   [kenich, div, many, tim, bal, man, sav, 50, rest, go, bound]\n",
       "3                                            [whol, body, feel, itchy, lik, fir]\n",
       "4                                                          [kweside, whol, crew]\n",
       "Name: lancaster_sw, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['lancaster_sw'] = df['lancaster'].apply(remove_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Stop words removed, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['lancaster_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                            [wok, hav, school, best, feel, ev]\n",
       "1309536               [thewdb, com, cool, hear, old, walt, interview]\n",
       "1309537                    [ar, ready, yo, mojo, makeov, ask, detail]\n",
       "1309538         [happy, 38th, birthday, boo, tim, tupac, amaru, shak]\n",
       "1309539    [happy, charitytuesday, thenspcc, sparksch, speakinguph4h]\n",
       "Name: lancaster_sw, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lancaster_sw'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def tokenizer_lemmatizer(text):\n",
    "    return [wnl.lemmatize(word) for word in text.split()]\n",
    "tokenizer_lemmatizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents were split into tokens, took 102.53 seconds (1.71 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                            [switchfoot, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d;D]\n",
       "1    [is, upset, that, he, can, t, update, his, facebook, by, texting, it, and, might, cry, a, a, result, school, today, also, blah]\n",
       "2                             [kenichan, i, dived, many, time, for, the, ball, managed, to, save, 50, the, rest, go, out, of, bound]\n",
       "3                                                                            [my, whole, body, feel, itchy, and, like, it, on, fire]\n",
       "4                                                                                                  [kwesidei, not, the, whole, crew]\n",
       "Name: wnl, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['wnl'] = df['text'].apply(tokenizer_lemmatizer)\n",
    "elapsed = time() - t\n",
    "print(\"Documents were split into tokens, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['wnl'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535            [just, woke, up, having, no, school, is, the, best, feeling, ever]\n",
       "1309536                     [thewdb, com, very, cool, to, hear, old, walt, interview]\n",
       "1309537            [are, you, ready, for, your, mojo, makeover, ask, me, for, detail]\n",
       "1309538    [happy, 38th, birthday, to, my, boo, of, alll, time, tupac, amaru, shakur]\n",
       "1309539               [happy, charitytuesday, thenspcc, sparkscharity, speakinguph4h]\n",
       "Name: wnl, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wnl'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet lemmatizer with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removed, took 46.74 seconds (0.78 minutes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0               [switchfoot, awww, bummer, shoulda, got, david, carr, third, day, d;D]\n",
       "1    [upset, update, facebook, texting, might, cry, result, school, today, also, blah]\n",
       "2              [kenichan, dived, many, time, ball, managed, save, 50, rest, go, bound]\n",
       "3                                               [whole, body, feel, itchy, like, fire]\n",
       "4                                                              [kwesidei, whole, crew]\n",
       "Name: wnl_sw, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "df['wnl_sw'] = df['wnl'].apply(remove_sw)\n",
    "elapsed = time() - t\n",
    "print(\"Stop words removed, took {0:,.2f} seconds ({1:,.2f} minutes)\"\n",
    "      .format(elapsed, elapsed / 60))\n",
    "df['wnl_sw'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1309535                                [woke, school, best, feeling, ever]\n",
       "1309536                    [thewdb, com, cool, hear, old, walt, interview]\n",
       "1309537                               [ready, mojo, makeover, ask, detail]\n",
       "1309538     [happy, 38th, birthday, boo, alll, time, tupac, amaru, shakur]\n",
       "1309539    [happy, charitytuesday, thenspcc, sparkscharity, speakinguph4h]\n",
       "Name: wnl_sw, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wnl_sw'].tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
