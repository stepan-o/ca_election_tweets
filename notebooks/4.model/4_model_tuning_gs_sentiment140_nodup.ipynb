{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019 Canadian Election tweets\n",
    "# OSEMN Step 4: Model\n",
    "# Sentiment analysis of Sentiment 140 dataset\n",
    "# Hyperparameter tuning: grid search of text vectorization methods\n",
    "\n",
    "This notebook describes part of Step 4: Explore of OSEMN methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import NaiveBayesClassifier\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "\n",
    "sns.set()\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git', '.gitignore', 'src', 'notebooks', 'methodology', 'README.md', 'data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../src')\n",
    "from proc_utils import string_concat, tfm_2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testdata.manual.2009.06.14.csv',\n",
       " 'training.1600000.processed.noemoticon.csv',\n",
       " 'sentiment140_train_nodup.csv',\n",
       " 'sentiment140_train_cleaned.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../data/sentiment140/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned Sentiment 140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- DataFrame loaded\n",
      "in 5.04 seconds\n",
      "with 1,309,540 rows\n",
      "and 8 columns\n",
      "-- Column names:\n",
      " Index(['sentiment', 'ids', 'date', 'query', 'user', 'text', 'hashtags',\n",
      "       'handles'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "df = pd.read_csv(data_dir + 'sentiment140_train_nodup.csv')\n",
    "elapsed = time() - t\n",
    "print(\"----- DataFrame loaded\"\n",
    "      \"\\nin {0:.2f} seconds\".format(elapsed) +\n",
    "      \"\\nwith {0:,} rows\\nand {1:,} columns\"\n",
    "      .format(df.shape[0], df.shape[1]) +\n",
    "      \"\\n-- Column names:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df['sentiment'] == 4\n",
    "df.loc[mask1, 'sentiment'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for tokenization / stemming, with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(w) for w in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball = EnglishStemmer()\n",
    "def tokenizer_snowball(text):\n",
    "    return [snowball.stem(w) for w in text.split()]\n",
    "tokenizer_snowball('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'lik', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "def tokenizer_lancaster(text):\n",
    "    return [lancaster.stem(w) for w in text.split()]\n",
    "tokenizer_lancaster('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def tokenizer_lemmatizer(text):\n",
    "    return [wnl.lemmatize(w) for w in text.split()]\n",
    "tokenizer_lemmatizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset for grid search: 1/5 of all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261,908 rows in the subset\n"
     ]
    }
   ],
   "source": [
    "s = df.sample(len(df) // 5, random_state=random_state).copy()\n",
    "print(\"{0:,.0f} rows in the subset\".format(len(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features and target, perform train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    135523\n",
       "1    126385\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train-test split.\n",
      "Labels counts in y: [135523 126385]\n",
      "Labels counts in y_train: [94866 88469]\n",
      "Labels counts in y_test: [40657 37916]\n"
     ]
    }
   ],
   "source": [
    "X = s['text']\n",
    "y = s['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n",
    "print(\"Performed train-test split.\")\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grid search of text vectorization hyperparameters\n",
    "Lemmatizer is currently not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_lr_tfidf.csv',\n",
       " 'gs_lr_bow.csv',\n",
       " 'gs_tree_bow.csv',\n",
       " 'gs_tree_tfidf.csv',\n",
       " 'gs_complnb_tfidf.csv',\n",
       " 'gs_lsvc_tfidf.csv',\n",
       " 'gs_multinb_tfidf.csv',\n",
       " 'gs_lsvc_bow.csv',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_save_dir = 'results/gs_results/'\n",
    "os.listdir(gs_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')  # corpus of English stopwords needs to be downloaded from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'vect__ngram_range': [(1,1), (1,2), (1,3)], \n",
    "     'vect__stop_words': [stop, None],\n",
    "     'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_lancaster],\n",
    "     'vect__binary': [True, False],\n",
    "     'vect__use_idf': [False],       # model based on raw term frequencies\n",
    "     'vect__smooth_idf': [False],    # model based on raw term frequencies\n",
    "     'vect__norm': [None]},          # model based on raw term frequencies\n",
    "    \n",
    "    {'vect__ngram_range': [(1,1), (1,2), (1,3)], \n",
    "     'vect__stop_words': [stop, None],\n",
    "     'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_lancaster],\n",
    "     'vect__binary': [True, False],\n",
    "     'vect__use_idf': [True],        # model based on TF-IDF\n",
    "     'vect__smooth_idf': [True],     # model based on TF-IDF\n",
    "     'vect__norm': ['l2']}           # model based on TF-IDF\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  2.2min\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=12)]: Done 480 out of 480 | elapsed: 47.5min finished\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed! Took 2,896.60 seconds (48.28 minutes)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), \n",
    "                     ('clf', LogisticRegression(random_state=random_state, penalty='l1', C=1.0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=12)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "elapsed = time() - t\n",
    "print(\"Grid search completed! Took {0:,.2f} seconds ({1:,.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022908882646521"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022908882646521"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__smooth_idf': True,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer_snowball(text)>,\n",
       " 'vect__use_idf': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8052893\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.7f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with grid search results recorded to a .csv file\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf_gs_results = pd.DataFrame(gs_lr_tfidf.cv_results_)\n",
    "lr_tfidf_gs_results.to_csv(gs_save_dir + 'gs_lr.csv', index=False)\n",
    "print(\"DataFrame with grid search results recorded to a .csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  2.2min\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=12)]: Done 480 out of 480 | elapsed: 51.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed! Took 3,115.76 seconds (51.93 minutes)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "lsvc_tfidf = Pipeline([('vect', tfidf), \n",
    "                       ('clf', LinearSVC(random_state=random_state, penalty='l2', C=0.1))])\n",
    "\n",
    "gs_lsvc_tfidf = GridSearchCV(lsvc_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=12)\n",
    "gs_lsvc_tfidf.fit(X_train, y_train)\n",
    "\n",
    "elapsed = time() - t\n",
    "print(\"Grid search completed! Took {0:,.2f} seconds ({1:,.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052635885128318"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lsvc_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__norm': None,\n",
       " 'vect__smooth_idf': False,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer(text)>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lsvc_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8083693\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lsvc_tfidf.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.7f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with grid search results recorded to a .csv file\n"
     ]
    }
   ],
   "source": [
    "lsvc_tfidf_gs_results = pd.DataFrame(gs_lsvc_tfidf.cv_results_)\n",
    "lsvc_tfidf_gs_results.to_csv(gs_save_dir + 'gs_lsvc.csv', index=False)\n",
    "print(\"DataFrame with grid search results recorded to a .csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutlinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  2.4min\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=12)]: Done 480 out of 480 | elapsed: 42.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed! Took 2,587.40 seconds (43.12 minutes)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "multinb_tfidf = Pipeline([('vect', tfidf), \n",
    "                          ('clf', MultinomialNB())])\n",
    "\n",
    "gs_multinb_tfidf = GridSearchCV(multinb_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=12)\n",
    "gs_multinb_tfidf.fit(X_train, y_train)\n",
    "elapsed = time() - t\n",
    "print(\"Grid search completed! Took {0:,.2f} seconds ({1:,.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7848365014863501"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_multinb_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__norm': None,\n",
       " 'vect__smooth_idf': False,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer(text)>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_multinb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7879424\n"
     ]
    }
   ],
   "source": [
    "clf = gs_multinb_tfidf.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.7f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with grid search results recorded to a .csv file\n"
     ]
    }
   ],
   "source": [
    "multinb_tfidf_gs_results = pd.DataFrame(gs_multinb_tfidf.cv_results_)\n",
    "multinb_tfidf_gs_results.to_csv(gs_save_dir + 'gs_multinb.csv', index=False)\n",
    "print(\"DataFrame with grid search results recorded to a .csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 15.2min\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 37.5min\n",
      "[Parallel(n_jobs=12)]: Done 480 out of 480 | elapsed: 42.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed! Took 2,580.27 seconds (43.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "complnb_tfidf = Pipeline([('vect', tfidf), \n",
    "                          ('clf', ComplementNB())])\n",
    "\n",
    "gs_complnb_tfidf = GridSearchCV(complnb_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=12)\n",
    "gs_complnb_tfidf.fit(X_train, y_train)\n",
    "elapsed = time() - t\n",
    "print(\"Grid search completed! Took {0:,.2f} seconds ({1:,.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7859437641475987"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_complnb_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 2),\n",
       " 'vect__norm': None,\n",
       " 'vect__smooth_idf': False,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer(text)>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_complnb_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7887315\n"
     ]
    }
   ],
   "source": [
    "clf = gs_complnb_tfidf.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.7f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with grid search results recorded to a .csv file\n"
     ]
    }
   ],
   "source": [
    "complnb_tfidf_gs_results = pd.DataFrame(gs_complnb_tfidf.cv_results_)\n",
    "complnb_tfidf_gs_results.to_csv(gs_save_dir + 'gs_complnb.csv', index=False)\n",
    "print(\"DataFrame with grid search results recorded to a .csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  4.0min\n",
      "/home/stepan/anaconda3/envs/twitter/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 82.9min\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 234.3min\n",
      "[Parallel(n_jobs=12)]: Done 480 out of 480 | elapsed: 293.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed! Took 18,338.24 seconds (305.64 minutes)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "tree_tfidf = Pipeline([('vect', tfidf), \n",
    "                       ('clf', DecisionTreeClassifier(random_state=random_state, \n",
    "                                                      criterion='gini', max_depth=40))])\n",
    "\n",
    "gs_tree_tfidf = GridSearchCV(tree_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=12)\n",
    "gs_tree_tfidf.fit(X_train, y_train)\n",
    "elapsed = time() - t\n",
    "print(\"Grid search completed! Took {0:,.2f} seconds ({1:,.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7010827174298415"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tree_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__binary': True,\n",
       " 'vect__ngram_range': (1, 3),\n",
       " 'vect__norm': None,\n",
       " 'vect__smooth_idf': False,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer_lancaster(text)>,\n",
       " 'vect__use_idf': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tree_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7039441\n"
     ]
    }
   ],
   "source": [
    "clf = gs_tree_tfidf.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test accuracy: %.7f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with grid search results recorded to a .csv file\n"
     ]
    }
   ],
   "source": [
    "tree_tfidf_gs_results = pd.DataFrame(gs_tree_tfidf.cv_results_)\n",
    "tree_tfidf_gs_results.to_csv(gs_save_dir + 'gs_tree.csv', index=False)\n",
    "print(\"DataFrame with grid search results recorded to a .csv file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
